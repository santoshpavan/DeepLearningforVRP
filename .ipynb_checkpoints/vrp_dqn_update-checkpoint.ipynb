{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "0R6chiOssBaj",
    "outputId": "42306fe1-4642-4702-a61f-1efb41333fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"vrp_dqn.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1iothquIlGtKKte5KIxO-YCKzXnZGjbK-\n",
    "\"\"\"\n",
    "# !pip install tsplib95\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import tsplib95\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.python.keras import Sequential\n",
    "# from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "import io # to save the image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZA_bOlXdsHOh",
    "outputId": "095084ca-9e58-4121-e0cc-117ee20db7ca"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9iofX44xw0s"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "class LearningRateReducerCb(tensorflow.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # old_lr = self.model.optimizer.lr.read_value()\n",
    "        # new_lr = old_lr * 0.99\n",
    "        new_lr = 0.1\n",
    "        # print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "        self.model.optimizer.lr.assign(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8NT6EoJDsQ4"
   },
   "outputs": [],
   "source": [
    "# 6. Define the movement restriction of the truck.\n",
    "class Truck:\n",
    "    def __init__(self, capacity, id, color):\n",
    "        self.id = id\n",
    "        self.color = color\n",
    "        self.path = [] #this has the list of nodes it has visited\n",
    "        self.max_truck_capacity = copy.deepcopy(capacity) #the max capacity\n",
    "        self.capacity = copy.deepcopy(capacity)\n",
    "        #self.visit_depo()\n",
    "        self.prev_node = None\n",
    "        self.node = 1 #starts from the depo\n",
    "\n",
    "    def action(self, choice):\n",
    "        # the number of choice of actions are the number of nodes-1\n",
    "        # the choice to be taken depends on the demands - penalty based\n",
    "        # the choice number is the same as the node number\n",
    "        # it is not a choice if the demand is 0 - changing this to penalty\n",
    "        #!! Want the system to learn instead\n",
    "        # if self.capacity == 0:\n",
    "        #     self.visit_depo()\n",
    "        self.move(choice)\n",
    "\n",
    "    def move(self, to_node_value):\n",
    "        # node_list_copy = copy.deepcopy(node_list)\n",
    "        # node_list_copy.remove(1)\n",
    "        # select a random node to go to\n",
    "        #if not to_node_value: #to_node_value is False by default\n",
    "        #    to_node_value = random.choice(self.node_list)\n",
    "        if to_node_value == 1:\n",
    "            self.visit_depo()\n",
    "        self.prev_node = self.node\n",
    "        self.node = to_node_value\n",
    "        self.path.append(to_node_value)\n",
    "        # when invoked update the demand of the node\n",
    "        # update the demand of the node\n",
    "\n",
    "    def visit_depo(self):\n",
    "        self.prev_node = self.node\n",
    "        self.node = 1 #here it is 1\n",
    "        self.capacity = copy.deepcopy(self.max_truck_capacity) #truck capacity reset\n",
    "        self.path.append(1)\n",
    "    \n",
    "    #def path(self, node_value):\n",
    "    #    self.path.append(node_value)\n",
    "\n",
    "    # def get_node(self):\n",
    "    #     return self.node\n",
    "\n",
    "    \n",
    "    # def get_capacity(self):\n",
    "    #     return self.capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjYDwT6rD15Q"
   },
   "outputs": [],
   "source": [
    "class VRPEnvironment:\n",
    "    # environment related constants\n",
    "    #https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.figure.html\n",
    "    #observation_space = (640,480,3)\n",
    "    # observation_space = (100,100,3)\n",
    "    observation_space = (10,10,3)\n",
    "    # penalty and rewards\n",
    "    non_positive_capacity_penalty = 100\n",
    "    zero_demand_penalty = 100 #truck goes to a zero demand node - except 1\n",
    "    # ignore the failing task penalty as the episode will end only when...\n",
    "    # all the demands are satisfied i.e. task will always be success...\n",
    "    # The penalty of the achievement needs to go down.\n",
    "    failing_task_penalty = 100 #trucks fail to complete the task\n",
    "    completion_reward = 5000 #trucks complete the task\n",
    "    demand_satisfying_reward = 100 #another incentive to hit the right target. Imp with considering each step\n",
    "    hopping_incentive_penalty = 500 #staying at the same node\n",
    "    # visit_correct_node_reward = 100\n",
    "    # exploration settings\n",
    "    permitted_path_length = 20 #30\n",
    "    epsilon = 1\n",
    "    epsilon_decay = 0.999#changing this from 0.999\n",
    "    min_epsilon = 0.001 #0.001\n",
    "    no_of_episodes = 10_000 #30_000\n",
    "    # from 0 to 0.5 difference is small so using 1 first\n",
    "    truck_colors = {\n",
    "        1:(0,0,1),\n",
    "        2:(0,1,0),\n",
    "        3:(1,0,0),\n",
    "        4:(0,0.5,0.5),\n",
    "        5:(0.5,0,0.5),\n",
    "        6:(0.5,0.5,0),\n",
    "        7:(0.5,0.5,0.5),\n",
    "        8:(0.5,0.5,1),\n",
    "        9:(0.5,1,0.5),\n",
    "        10:(1,0.5,0.5)\n",
    "    }\n",
    "    return_images = True\n",
    "    image_size = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        # 1. Extract the tsplib95 file problem\n",
    "        # self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/A/A-n32-k5.vrp')\n",
    "        # self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/_singleTruck/A-n32-k5_2.vrp')\n",
    "        self.problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n32-k5_3.vrp')        \n",
    "        # 2. Create a networkx graph out of the problem. //will be plotting this\n",
    "        self.nx_graph = self.problem.get_graph()\n",
    "        self.edge_list = list(self.problem.get_edges()) #[(,)]\n",
    "        self.node_positions = self.problem.node_coords #dict\n",
    "        # the list of nodes\n",
    "        self.node_list = list(self.problem.get_nodes())\n",
    "        self.action_space = len(self.node_list) #the number of choices including staying put\n",
    "        # the depot location\n",
    "        self.depot_location = 1\n",
    "        # assigning the default color of the nodes to be black\n",
    "        # node_color_values = [(0,0,0) for i in range(len(node_list))]\n",
    "        # reseting the environment when initialized\n",
    "        self.reset_environment()\n",
    "        \n",
    "    def reset_environment(self):\n",
    "        # creating the Trucks\n",
    "        # 4. Extract the necessary data about the trucks. //no of trucks, depot_section, capacity\n",
    "        self.node_demands = copy.deepcopy(self.problem.demands)\n",
    "        truck_capacity = copy.deepcopy(self.problem.capacity)\n",
    "        # trying hardcoding for now\n",
    "        self.truck = Truck(truck_capacity, 1, self.truck_colors.get(3))\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.return_images:\n",
    "            observation = np.array(self.get_image())\n",
    "        \n",
    "        # resetting the environment reward value\n",
    "        # self.reward = 0\n",
    "        # there is no else case as we need always need the image for CNN\n",
    "        return observation\n",
    "\n",
    "    # def calculate_move_penalty(self, source_node, dest_node):\n",
    "    #     return self.problem.wfunc(source_node, dest_node) #the weight of the edge\n",
    "\n",
    "    # change the demand of the node when visited\n",
    "    def change_demand(self, node):\n",
    "        self.node_demands[node] = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        # moving the trucks for the action\n",
    "        self.truck.action(action)\n",
    "        #print(\"action=\"+str(action)+\"\\n++++++\")\n",
    "        self.truck.capacity -= self.node_demands.get(action)\n",
    "        self.change_demand(action)\n",
    "        # print(self.truck.capacity)\n",
    "        # other truck actions\n",
    "\n",
    "        if self.return_images:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        # there is no else case as the return image is always true\n",
    "\n",
    "        # assinging the rewards and penalties\n",
    "        self.reward = 0\n",
    "        # checking if the demands have been satisfied\n",
    "        completed = False\n",
    "        if sum(list(self.node_demands.values())) == 0:\n",
    "            print(\"\\n***Satisfied***\")\n",
    "            # print(self.node_demands)\n",
    "            completed = True\n",
    "            self.reward = self.completion_reward\n",
    "        else:\n",
    "            # rewards for other trucks\n",
    "            # print(\"before\"+str(self.reward))\n",
    "            self.node_penalty(self.truck) #other penalties\n",
    "            self.movement_penalty(self.truck) #edge weight\n",
    "            # print(\"after\"+str(self.reward))\n",
    "            # penalties for other trucks\n",
    "        done = False\n",
    "        if self.reward == self.completion_reward or len(self.truck.path) >= self.permitted_path_length:\n",
    "        # if self.reward == self.completion_reward:\n",
    "            done = True\n",
    "            if sum(list(self.node_demands.values())) > 0:\n",
    "                self.reward -= self.failing_task_penalty\n",
    "\n",
    "        return new_observation, self.reward, done, completed\n",
    "\n",
    "    def node_penalty(self, truck):\n",
    "        if self.node_demands[truck.node] == 0:\n",
    "            if (truck.node == 1 and truck.capacity == truck.max_truck_capacity) or truck.node!=1:\n",
    "                self.reward -= self.zero_demand_penalty\n",
    "        else:\n",
    "            self.reward += self.demand_satisfying_reward\n",
    "\n",
    "        if self.truck.capacity <= 0:\n",
    "            self.reward -= self.non_positive_capacity_penalty\n",
    "        # return self.reward\n",
    "\n",
    "    def movement_penalty(self, truck):\n",
    "        #print(self.truck.path)\n",
    "        #print(\"-------\")\n",
    "        if truck.prev_node: #else it's 0\n",
    "            source_node = truck.prev_node\n",
    "            destination_node = truck.node\n",
    "            if source_node == destination_node: #if truck stays at the same place\n",
    "                self.reward -= self.hopping_incentive_penalty\n",
    "            self.reward -= self.problem.wfunc(source_node, destination_node)\n",
    "            # return self.problem.wfunc(source_node, destination_node)\n",
    "        # else:\n",
    "        #     source_node = truck.node\n",
    "        #     destination_node = truck.node\n",
    "        #     return 0\n",
    "    # \"\"\"\n",
    "    def get_image(self):\n",
    "        # the initiated rgb image of the given size. image_size = 100\n",
    "        env = np.zeros((self.image_size, self.image_size, 3), dtype=np.uint8)\n",
    "        for node in self.node_positions.keys():\n",
    "            node_coods = self.node_positions.get(node)\n",
    "            env[int(node_coods[0])][int(node_coods[1])] = (255,255,255)\n",
    "        if self.truck.path: #if there are elements in the path\n",
    "            for visited_node in set(self.truck.path):\n",
    "                node_coods = self.node_positions.get(visited_node)\n",
    "                #print(node_coods)\n",
    "                #print(visited_node)\n",
    "                #print(self.truck.path)\n",
    "                #print(len(env))\n",
    "                #print(\"--------\")\n",
    "                #try: #HAVING 0 HERE WAS CAUSING ERRORS! IGNORING 0 FOR NOW- STUPID\n",
    "                env[int(node_coods[0])][int(node_coods[1])] = (255,0,0)\n",
    "                #except:\n",
    "                    #print(node_coods)\n",
    "                    #print(visited_node)\n",
    "                    #print(self.truck.path)\n",
    "                    #print(len(env))\n",
    "                    #print(\"--------\")\n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        # trying to reduce to size to decrease the time taken\n",
    "        img = img.resize((10,10))\n",
    "        return img\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((500,500))\n",
    "        cv2.imshow(\"image\", np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "    \"\"\"\n",
    "    def get_image(self):\n",
    "        fig,ax = plt.subplots()\n",
    "        node_color_values = [(0,0,1) for i in range(len(self.node_list))]\n",
    "        # ax.clear()\n",
    "        if not self.truck.path: #if it is not empty\n",
    "            for i in self.truck.path:\n",
    "                node_color_values[i] = (1,0,0)\n",
    "        image_io = io.BytesIO()\n",
    "        #edge color is white to ignore it\n",
    "        nx.draw(self.nx_graph, pos=self.node_positions, with_labels=False, node_color=node_color_values, node_size=20, edge_color=(1,1,1))\n",
    "        fig.savefig(image_io, dpi=5)\n",
    "        image = Image.open(image_io)\n",
    "        image = image.resize((10,10))\n",
    "        image.close() #keeping it open consumes a lot of memory\n",
    "        return image\n",
    "\n",
    "    def render(self):\n",
    "        image = self.get_image()\n",
    "        image = image.resize((1000,1000))\n",
    "        cv2.imshow(\"image\",np.array(image))\n",
    "        cv2.waitKey(1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjqrxI5rr3s_"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # main model\n",
    "        self.main_model = self.create_model()\n",
    "        \n",
    "        # target model\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "        # an array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=considering_training_length)\n",
    "\n",
    "        # used to know when to update target n/w with main n/w's weights\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # model.add(Conv2D(256, (10,10), input_shape=environment.observation_space))\n",
    "        model.add(Conv2D(256, (3,3), input_shape=environment.observation_space))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # model.add(Conv2D(256,(10,10)))\n",
    "        model.add(Conv2D(256,(3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # converting the 3D features into the 1D feature\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(environment.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # adding the step data into the array to be considered later\n",
    "    def update_replay_memory(self, step):\n",
    "        #print(step[2])\n",
    "        #print(\"****\")\n",
    "        self.replay_memory.append(step)\n",
    "        \n",
    "    def consider_reward(self, element):\n",
    "        return element[1]\n",
    "        \n",
    "    def get_max_rewarded_steps(self, replay_memory, length):\n",
    "        #replay_memory has the steps and steps[2] is the rewards\n",
    "        # max = -math.inf\n",
    "        step_reward_pairs = []\n",
    "        for step in replay_memory:\n",
    "            if len(step_reward_pairs) <= length:\n",
    "                step_reward_pairs.append((step, step[2]))\n",
    "            else:\n",
    "                step_reward_pairs.sort(reverse=True, key=self.consider_reward)\n",
    "                if step_reward_pairs[-1][1] < step[2]:\n",
    "                    step_reward_pairs.pop() #removing the least value i.e. the last value\n",
    "                    step_reward_pairs.append((step, step[2]))\n",
    "        steps_list = []\n",
    "        for step_reward_pair in step_reward_pairs:\n",
    "            steps_list.append(step_reward_pair[0])\n",
    "        return steps_list\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "        # start training only when we have a certain number of samples already saved\n",
    "        if len(self.replay_memory)< min_replay_memory_size:\n",
    "            return\n",
    "        # get the minibatch of the samples from the replay table - OLD\n",
    "        #minibatch = random.sample(self.replay_memory, min_training_length)\n",
    "        # get the minibatch of the samples that have the highest reward\n",
    "        minibatch = self.get_max_rewarded_steps(self.replay_memory, min_training_length)\n",
    "        \n",
    "        # get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.main_model.predict(current_states)\n",
    "\n",
    "        # get future states from the minibatch, then query NN model for Q values\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # enumerating through the batches\n",
    "        for index,(current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            # if its not a terminal state, get new q from future states or else set to 0\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + discount*max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # update the q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # append to the training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "        \n",
    "        # fit on all samples as one batch\n",
    "        self.main_model.fit(np.array(X)/255, np.array(y), batch_size=min_training_length, verbose=0, shuffle=False, callbacks=[LearningRateReducerCb()])\n",
    "        \n",
    "        # updating the target n/w counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # if the counter reaches the required value...\n",
    "        # update the target n/w with weights of main n/w\n",
    "        if self.target_update_counter > update_target_every:\n",
    "            self.target_model.set_weights(self.main_model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "    \n",
    "    # query the main n/w for q values given the current observation space\n",
    "    def get_qs(self, state):\n",
    "        return self.main_model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model related constants\n",
    "discount = 0.99 #the discount applied to the DQN equation\n",
    "# the environment keeps running till the demand of the nodes is satisfied\n",
    "considering_training_length = 50_000 #the no of steps considered for training\n",
    "min_training_length = 100 #the no of steps used for training\n",
    "# episodes = 30_000\n",
    "update_target_every = 5 #terminal states (end of episodes)\n",
    "min_replay_memory_size = 1000 #min no of steps in a memory to start training\n",
    "\n",
    "environment = VRPEnvironment()\n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bb223c95saFs",
    "outputId": "f5525649-01fb-4d00-8df6-0a5ffe528010"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                             | 0/10000 [00:00<?, ?episodes/s]\u001b[A\n",
      "  0%|3                                                                                                                                                                  | 20/10000 [00:00<00:52, 189.68episodes/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is: [6, 1, 1, 4, 7, 8, 9, 8, 8, 3, 5, 6, 9, 3, 7, 7, 1, 1, 3, 5];P and R: -3953;Capacity: 100;Epsilon: 0.9996667222160499\n",
      "Path is: [5, 2, 5, 2, 1, 1, 3, 4, 4, 2, 6, 7, 2, 1, 1, 1, 1, 1, 1, 4];P and R: -4243;Capacity: 100;Epsilon: 0.9993335555061811\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [6, 9, 6, 9, 2, 7, 5, 4, 7, 5, 6, 2, 3, 3, 7, 4, 5, 3, 2, 8];P and R: 2531;Capacity: 2;Epsilon: 0.999000499833375\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 2, 1, 1, 7, 9, 5, 9, 6, 6, 1, 1, 4, 9, 9, 3, 8];P and R: 1543;Capacity: 61;Epsilon: 0.9986675551606254\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [4, 6, 8, 5, 1, 1, 7, 2, 1, 1, 2, 9];P and R: 3070;Capacity: 99;Epsilon: 0.9983347214509387\n",
      "Path is: [6, 2, 5, 3, 9, 6, 4, 6, 5, 9, 9, 4, 8, 2, 5, 4, 3, 1, 1, 9];P and R: -3081;Capacity: 100;Epsilon: 0.9980019986673331\n",
      "Path is: [6, 8, 2, 7, 8, 2, 4, 8, 1, 1, 4, 2, 6, 9, 6, 6, 7, 4, 9, 7];P and R: -3077;Capacity: 99;Epsilon: 0.9976693867728395\n",
      "Path is: [6, 1, 1, 8, 3, 4, 4, 5, 1, 1, 1, 1, 4, 1, 1, 8, 3, 6, 3, 5];P and R: -4258;Capacity: 100;Epsilon: 0.9973368857305009\n",
      "Path is: [3, 9, 7, 9, 1, 1, 6, 1, 1, 8, 8, 8, 2, 7, 8, 8, 8, 2, 2, 6];P and R: -5462;Capacity: 74;Epsilon: 0.997004495503373\n",
      "Path is: [6, 6, 8, 3, 8, 5, 6, 1, 1, 7, 4, 5, 3, 6, 6, 4, 1, 1, 9, 9];P and R: -4463;Capacity: 99;Epsilon: 0.9966722160545233\n",
      "Path is: [1, 1, 8, 3, 4, 2, 5, 5, 5, 8, 1, 1, 5, 8, 2, 5, 4, 2, 5, 8];P and R: -3963;Capacity: 100;Epsilon: 0.9963400473470321\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 6, 9, 4, 2, 5, 4, 7, 1, 1, 8];P and R: 3571;Capacity: 82;Epsilon: 0.9960079893439915\n",
      "Path is: [8, 4, 5, 6, 4, 3, 5, 2, 2, 6, 8, 3, 9, 1, 1, 5, 8, 2, 2, 2];P and R: -4071;Capacity: 100;Epsilon: 0.9956760420085063\n",
      "Path is: [9, 1, 1, 5, 5, 2, 4, 7, 7, 1, 1, 3, 2, 7, 7, 6, 9, 1, 1, 4];P and R: -4848;Capacity: 100;Epsilon: 0.9953442053036935\n",
      "Path is: [2, 2, 1, 1, 8, 8, 5, 4, 9, 8, 8, 9, 2, 7, 8, 8, 7, 7, 4, 7];P and R: -5050;Capacity: 34;Epsilon: 0.9950124791926823\n",
      "Path is: [4, 8, 3, 5, 4, 8, 7, 1, 1, 5, 1, 1, 4, 2, 8, 4, 5, 7, 2, 8];P and R: -2974;Capacity: 92;Epsilon: 0.9946808636386143\n",
      "Path is: [9, 2, 3, 1, 1, 6, 2, 1, 1, 1, 1, 2, 1, 1, 9, 7, 7, 4, 4, 4];P and R: -5252;Capacity: 67;Epsilon: 0.9943493586046432\n",
      "Path is: [8, 5, 9, 9, 6, 5, 1, 1, 2, 7, 1, 1, 9, 5, 9, 6, 2, 8, 5, 7];P and R: -3478;Capacity: 100;Epsilon: 0.9940179640539353\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 9, 2, 5, 4, 9, 9, 4, 2, 5, 9, 9, 1, 1, 7, 6, 3, 7, 8];P and R: 1232;Capacity: 46;Epsilon: 0.9936866799496687\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [4, 7, 5, 1, 1, 6, 3, 3, 4, 7, 8, 8, 3, 9, 3, 9, 3, 6, 3, 2];P and R: 1625;Capacity: 49;Epsilon: 0.9933555062550344\n",
      "Path is: [1, 1, 3, 6, 5, 5, 1, 1, 6, 5, 6, 1, 1, 1, 1, 9, 9, 5, 1, 1];P and R: -5133;Capacity: 100;Epsilon: 0.9930244429332351\n",
      "Path is: [9, 4, 3, 4, 5, 5, 1, 1, 3, 9, 6, 3, 4, 5, 7, 6, 5, 9, 5, 2];P and R: -3060;Capacity: 56;Epsilon: 0.9926934899474861\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [4, 2, 5, 6, 9, 7, 4, 4, 7, 4, 8];P and R: 3470;Capacity: 2;Epsilon: 0.9923626472610146\n",
      "Path is: [3, 8, 8, 1, 1, 3, 8, 6, 6, 5, 3, 7, 9, 8, 3, 7, 5, 8, 2, 9];P and R: -3573;Capacity: 41;Epsilon: 0.9920319148370607\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [6, 5, 3, 7, 9, 1, 1, 4, 7, 8, 6, 2];P and R: 3469;Capacity: 53;Epsilon: 0.991701292638876\n",
      "Path is: [5, 9, 1, 1, 9, 9, 6, 4, 7, 2, 7, 9, 1, 1, 6, 7, 3, 5, 5, 6];P and R: -3967;Capacity: 100;Epsilon: 0.9913707806297248\n",
      "Path is: [9, 7, 5, 4, 5, 4, 2, 1, 1, 6, 9, 3, 9, 5, 7, 1, 1, 5, 9, 2];P and R: -2977;Capacity: 100;Epsilon: 0.9910403787728836\n",
      "Path is: [2, 2, 7, 7, 2, 3, 9, 1, 1, 4, 4, 2, 2, 2, 9, 9, 9, 8, 2, 2];P and R: -6549;Capacity: 61;Epsilon: 0.9907100870316412\n",
      "Path is: [8, 7, 4, 6, 1, 1, 2, 7, 7, 6, 4, 7, 4, 6, 8, 9, 1, 1, 3, 9];P and R: -3456;Capacity: 100;Epsilon: 0.9903799053692982\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 6, 4, 8, 8, 5, 3, 4, 7, 8, 5, 9, 7, 8, 4, 6, 6, 2];P and R: 1750;Capacity: 2;Epsilon: 0.9900498337491681\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 7, 1, 1, 9, 2, 7, 4, 6, 1, 1, 1, 1, 8, 5];P and R: 2345;Capacity: 86;Epsilon: 0.989719872134576\n",
      "Path is: [3, 9, 9, 5, 1, 1, 8, 2, 4, 4, 8, 6, 8, 4, 4, 6, 1, 1, 3, 2];P and R: -4462;Capacity: 100;Epsilon: 0.9893900204888596\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 7, 6, 8, 2, 6, 9, 6, 3, 4];P and R: 4061;Capacity: 2;Epsilon: 0.9890602787753687\n",
      "Path is: [1, 1, 3, 3, 9, 8, 6, 9, 7, 1, 1, 9, 5, 3, 5, 8, 5, 1, 1, 6];P and R: -3858;Capacity: 100;Epsilon: 0.9887306469574654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|5                                                                                                                                                                  | 35/10000 [00:00<01:15, 132.03episodes/s]\u001b[A\n",
      "  0%|7                                                                                                                                                                  | 47/10000 [00:00<01:28, 111.89episodes/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is: [7, 7, 7, 1, 1, 7, 2, 8, 7, 8, 6, 3, 2, 6, 7, 1, 1, 2, 5, 5];P and R: -4453;Capacity: 86;Epsilon: 0.9884011249985238\n",
      "Path is: [2, 4, 9, 2, 4, 2, 4, 3, 7, 2, 1, 1, 3, 3, 5, 2, 7, 4, 1, 1];P and R: -3463;Capacity: 100;Epsilon: 0.9880717128619305\n",
      "Path is: [7, 6, 5, 5, 6, 1, 1, 9, 3, 9, 4, 8, 3, 8, 9, 4, 3, 1, 1, 9];P and R: -3483;Capacity: 100;Epsilon: 0.9877424105110841\n",
      "Path is: [4, 6, 3, 4, 4, 7, 5, 2, 7, 9, 2, 5, 1, 1, 5, 5, 7, 5, 2, 2];P and R: -4061;Capacity: 100;Epsilon: 0.9874132179093955\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 1, 1, 2, 9, 5, 2, 1, 1, 7, 2, 3, 8, 1, 1, 1, 1, 2, 4, 6];P and R: 1453;Capacity: 55;Epsilon: 0.9870841350202876\n",
      "Path is: [1, 1, 1, 1, 2, 2, 6, 3, 3, 1, 1, 1, 1, 6, 3, 5, 4, 7, 4, 2];P and R: -4737;Capacity: 53;Epsilon: 0.9867551618071957\n",
      "Path is: [9, 8, 6, 1, 1, 7, 1, 1, 3, 1, 1, 1, 1, 3, 2, 7, 7, 1, 1, 4];P and R: -4641;Capacity: 79;Epsilon: 0.9864262982335673\n",
      "Path is: [9, 2, 9, 6, 3, 8, 5, 5, 8, 5, 5, 1, 1, 8, 1, 1, 8, 2, 7, 3];P and R: -3997;Capacity: 88;Epsilon: 0.9860975442628619\n",
      "Path is: [7, 8, 3, 2, 6, 7, 6, 5, 4, 3, 5, 4, 2, 4, 2, 2, 5, 1, 1, 3];P and R: -3048;Capacity: 100;Epsilon: 0.9857688998585513\n",
      "Path is: [6, 1, 1, 4, 4, 6, 6, 8, 4, 5, 8, 9, 9, 3, 3, 9, 1, 1, 7, 3];P and R: -4959;Capacity: 88;Epsilon: 0.9854403649841196\n",
      "Path is: [5, 5, 6, 3, 4, 3, 6, 3, 6, 3, 7, 6, 2, 8, 6, 2, 7, 5, 6, 4];P and R: -2673;Capacity: 3;Epsilon: 0.9851119396030626\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 3, 9, 4, 2, 5, 7, 4, 1, 1, 9, 6];P and R: 3444;Capacity: 76;Epsilon: 0.9847836236788889\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 1, 1, 6, 9, 5, 3, 6, 5, 7, 8, 8, 3, 4, 4, 7, 4, 6, 2];P and R: 1743;Capacity: 20;Epsilon: 0.9844554171751189\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 4, 7, 6, 4, 8, 5, 3, 7, 6, 4, 2, 2, 6, 3, 9];P and R: 2944;Capacity: 2;Epsilon: 0.9841273200552851\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 5, 8, 7, 1, 1, 8, 7, 2, 1, 1, 6, 5, 8, 8, 8, 9, 5, 9, 4];P and R: 1249;Capacity: 54;Epsilon: 0.9837993322829324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|8                                                                                                                                                                   | 54/10000 [00:00<01:50, 90.01episodes/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 5, 4, 8, 9, 2, 5, 3, 4, 8, 8, 6, 1, 1, 6, 6, 2, 7];P and R: 1347;Capacity: 88;Epsilon: 0.9834714538216175\n",
      "Path is: [1, 1, 8, 1, 1, 5, 4, 5, 6, 9, 3, 8, 8, 9, 5, 9, 1, 1, 9, 2];P and R: -3869;Capacity: 92;Epsilon: 0.9831436846349096\n",
      "Path is: [6, 1, 1, 9, 4, 8, 3, 4, 3, 6, 3, 8, 8, 3, 5, 5, 8, 9, 9, 2];P and R: -4078;Capacity: 38;Epsilon: 0.98281602468639\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 7, 5, 2, 2, 4, 7, 8, 5, 9, 2, 9, 6];P and R: 2750;Capacity: 2;Epsilon: 0.9824884739396519\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 4, 9, 6, 5, 8, 3, 9, 2, 8, 7];P and R: 3946;Capacity: 2;Epsilon: 0.9821610323583008\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 2, 2, 5, 2, 7, 7, 4, 6, 5, 3, 9, 3, 8];P and R: 2643;Capacity: 2;Epsilon: 0.9818336999059541\n",
      "Path is: [2, 7, 3, 6, 1, 1, 6, 9, 1, 1, 2, 2, 9, 8, 1, 1, 5, 1, 1, 6];P and R: -4253;Capacity: 100;Epsilon: 0.981506476546242\n",
      "Path is: [4, 9, 4, 1, 1, 9, 4, 4, 7, 7, 1, 1, 9, 6, 2, 3, 3, 6, 5, 3];P and R: -4467;Capacity: 54;Epsilon: 0.981179362242806\n",
      "Path is: [3, 7, 2, 8, 3, 8, 4, 7, 9, 7, 2, 2, 3, 2, 6, 6, 9, 3, 9, 7];P and R: -3189;Capacity: 16;Epsilon: 0.9808523569593002\n",
      "Path is: [3, 4, 7, 1, 1, 5, 3, 8, 1, 1, 1, 1, 3, 4, 6, 9, 9, 1, 1, 6];P and R: -4242;Capacity: 100;Epsilon: 0.9805254606593905\n",
      "Path is: [4, 3, 8, 5, 3, 4, 1, 1, 6, 9, 2, 1, 1, 5, 6, 4, 4, 3, 1, 1];P and R: -3851;Capacity: 100;Epsilon: 0.9801986733067553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|#                                                                                                                                                                   | 61/10000 [00:03<22:58,  7.21episodes/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 5, 5, 6, 4, 3, 6, 6, 4, 2, 5, 7, 2, 8, 9];P and R: 2057;Capacity: 2;Epsilon: 0.9798719948650848\n",
      "Path is: [7, 3, 7, 5, 6, 6, 2, 7, 2, 9, 5, 3, 8, 5, 3, 8, 7, 8, 3, 5];P and R: -2694;Capacity: 23;Epsilon: 0.9795454252980814\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 1, 1, 3, 4, 5, 8, 2, 3, 5, 9, 6, 7];P and R: 3360;Capacity: 20;Epsilon: 0.9792189645694596\n",
      "Path is: [4, 4, 8, 5, 4, 6, 4, 3, 6, 3, 1, 1, 1, 1, 3, 1, 1, 6, 5, 6];P and R: -3842;Capacity: 100;Epsilon: 0.9788926126429459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-74124ebb82d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Every step we update replay memory and train main network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-da8f45250916>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, terminal_state, step)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# get future states from the minibatch, then query NN model for Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mnew_current_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtransition\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mfuture_qs_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_current_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \"\"\"\n\u001b[1;32m-> 1615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 3958\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   3959\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3960\u001b[0m       raise TypeError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mfunc_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     func_graph = FuncGraph(name, collections=collections,\n\u001b[1;32m--> 868\u001b[1;33m                            capture_by_value=capture_by_value)\n\u001b[0m\u001b[0;32m    869\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFuncGraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, collections, capture_by_value)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mouter\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfailing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m--> 184\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFuncGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2811\u001b[0m     \u001b[1;31m# requirement (many custom ops do not have shape functions, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2812\u001b[0m     \u001b[1;31m# want to break these existing cases).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2813\u001b[1;33m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetRequireShapeInferenceFns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2814\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2815\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to_thread_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "show_preview = False #using this will make it slow. Don't care about this now.\n",
    "aggregrate_stats_every = 50\n",
    "start_decaying = True #making this True ignores the conditional decaying - Notes 2.a\n",
    "\n",
    "x_axis = list(range(environment.no_of_episodes+1))\n",
    "epsilon_list = []\n",
    "rewards_list = []\n",
    "path_length_list = []\n",
    "\n",
    "epsilon_decaying_factor = 1500\n",
    "\n",
    "\n",
    "# iterate over the episodes\n",
    "for episode in tqdm(range(1, environment.no_of_episodes + 1), ascii=True, unit='episodes'):\n",
    "    #print(episode)\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    # Reset environment and get initial state\n",
    "    current_state = environment.reset_environment()\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > environment.epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "            #print(\"action=\"+str(action)+\"\\n++++++\")\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(1, environment.action_space)\n",
    "\n",
    "        if action != 0: #JUST IGNORING 0. MIGHT BE STUPID!\n",
    "            new_state, reward, done, completed = environment.step(action)\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "        if show_preview and not episode % aggregrate_stats_every:\n",
    "            env.render()\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "        # Decay the Epsilon\n",
    "        if completed:\n",
    "            start_decaying = True\n",
    "        \n",
    "        if start_decaying:\n",
    "            # environment.epsilon = 0.001\n",
    "            if environment.epsilon > environment.min_epsilon:\n",
    "                # environment.epsilon *= environment.epsilon_decay\n",
    "                environment.epsilon = np.exp(-episode/epsilon_decaying_factor)\n",
    "                environment.epsilon = max(environment.min_epsilon, environment.epsilon)\n",
    "        # print(\".\",end=\"\")\n",
    "    epsilon_list.append(environment.epsilon)\n",
    "    rewards_list.append(episode_reward)\n",
    "    path_length_list.append(len(environment.truck.path))\n",
    "    \n",
    "    print(\"Path is:\", end=\" \")\n",
    "    print(environment.truck.path, end=\";\")\n",
    "    print(\"P and R: \"+str(episode_reward), end=\";\")\n",
    "    print(\"Capacity: \"+str(environment.truck.capacity), end=\";\")\n",
    "    print(\"Epsilon: \"+str(environment.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBaJVfaDzf5x"
   },
   "outputs": [],
   "source": [
    "# # problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/A/A-n32-k5.vrp')\n",
    "# problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/_singleTruck/A-n32-k5_2.vrp')\n",
    "# nx_graph = problem.get_graph()\n",
    "# edge_list = list(problem.get_edges()) #[(,)]N\n",
    "# node_positions = problem.node_coords #dict\n",
    "# node_demands = copy.deepcopy(problem.demands)\n",
    "# truck_capacity = problem.capacity\n",
    "# Try rewarding for each node too i.e incentivize hopping!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWLdEBhZLRuR"
   },
   "outputs": [],
   "source": [
    "# node_demands[7] = 0\n",
    "# print(node_demands)\n",
    "# print(problem.demands)\n",
    "# print(sum(list(node_demands.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1u2PzUxLYsO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x236945c2780>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10000))\n",
    "# print(x)\n",
    "k = 1500\n",
    "y = [float(np.exp(-i/k)) for i in x]\n",
    "# print(y)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis,epsilon_list,'r')\n",
    "plt.plot(x_axis,rewards_list,'g')\n",
    "plt.plot(x_axis,path_length_list,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n32-k5_3.vrp')\n",
    "problem.wfunc(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (3, 2), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "class keka:\n",
    "    def test(self,i):\n",
    "        return i[1]\n",
    "    a = [(1,3),(2,1),(3,2)]\n",
    "    def uppu(self):\n",
    "        self.a.sort(reverse=True,key=self.test)\n",
    "        return self.a\n",
    "    \n",
    "k = keka()\n",
    "print(k.uppu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vrp_dqn_update.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
