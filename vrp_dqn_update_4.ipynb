{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "0R6chiOssBaj",
    "outputId": "10482e68-8290-4afc-80c4-1e35f0e59bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tsplib95==0.6.1 in c:\\users\\santo\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: networkx~=2.1 in c:\\users\\santo\\anaconda3\\lib\\site-packages (from tsplib95==0.6.1) (2.4)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\santo\\anaconda3\\lib\\site-packages (from tsplib95==0.6.1) (7.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\santo\\anaconda3\\lib\\site-packages (from networkx~=2.1->tsplib95==0.6.1) (4.4.1)\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"vrp_dqn.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1iothquIlGtKKte5KIxO-YCKzXnZGjbK-\n",
    "\"\"\"\n",
    "!pip install tsplib95==0.6.1\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import tsplib95\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.python.keras import Sequential\n",
    "# from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "import io # to save the image\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "z9GAR8RsZ0ks",
    "outputId": "587d51d3-5a19-4b6f-c7d0-d04cc68bfdcb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9iofX44xw0s"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "class LearningRateReducerCb(tensorflow.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # old_lr = self.model.optimizer.lr.read_value()\n",
    "        # new_lr = old_lr * 0.99\n",
    "        new_lr = 0.01\n",
    "        # print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "        self.model.optimizer.lr.assign(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8NT6EoJDsQ4"
   },
   "outputs": [],
   "source": [
    "# 6. Define the movement restriction of the truck.\n",
    "class Truck:\n",
    "    def __init__(self, capacity, id, color):\n",
    "        self.id = id\n",
    "        self.color = color\n",
    "        self.path = [] #this has the list of nodes it has visited\n",
    "        self.max_truck_capacity = copy.deepcopy(capacity) #the max capacity\n",
    "        self.capacity = copy.deepcopy(capacity)\n",
    "        #self.visit_depo()\n",
    "        self.prev_node = None\n",
    "        self.node = 1 #starts from the depo\n",
    "\n",
    "    def action(self, choice):\n",
    "        # the number of choice of actions are the number of nodes-1\n",
    "        self.move(choice)\n",
    "\n",
    "    def move(self, to_node_value):\n",
    "        if to_node_value == 1:\n",
    "            self.visit_depo()\n",
    "        self.prev_node = self.node\n",
    "        self.node = to_node_value\n",
    "        self.path.append(to_node_value)\n",
    "        # when invoked update the demand of the node\n",
    "        # update the demand of the node\n",
    "\n",
    "    def visit_depo(self):\n",
    "        self.prev_node = self.node\n",
    "        self.node = 1 #here it is 1\n",
    "        self.capacity = copy.deepcopy(self.max_truck_capacity) #truck capacity reset\n",
    "        self.path.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjYDwT6rD15Q"
   },
   "outputs": [],
   "source": [
    "class VRPEnvironment:\n",
    "    # environment related constants\n",
    "    #https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.figure.html\n",
    "    # observation_space = (640,480,3)\n",
    "    # observation_space = (100,100,3)\n",
    "    observation_space = (10,10,3)\n",
    "    # observation_space = (5,5,3)\n",
    "    # penalty and rewards\n",
    "    non_positive_capacity_penalty = 5000\n",
    "    zero_demand_penalty = 100 #truck goes to a zero demand node - except 1\n",
    "    # ignore the failing task penalty as the episode will end only when...\n",
    "    # all the demands are satisfied i.e. task will always be success...\n",
    "    # The penalty of the achievement needs to go down.\n",
    "    failing_task_penalty = 200 #trucks fail to complete the task\n",
    "    completion_reward = 5000 #trucks complete the task\n",
    "    demand_satisfying_reward = 100 #another incentive to hit the right target. Imp with considering each step\n",
    "    hopping_incentive_penalty = 500 #staying at the same node\n",
    "    # visit_correct_node_reward = 100\n",
    "    # exploration settings\n",
    "    permitted_path_length = 30 #30\n",
    "    epsilon = 1\n",
    "    # epsilon_decay = 0.999#changing this from 0.999\n",
    "    min_epsilon = 0.001 #0.001\n",
    "    no_of_episodes = 30_000 #30_000\n",
    "    # from 0 to 0.5 difference is small so using 1 first\n",
    "    truck_colors = {\n",
    "        1:(0,0,1),\n",
    "        2:(0,1,0),\n",
    "        3:(1,0,0),\n",
    "        4:(0,0.5,0.5),\n",
    "        5:(0.5,0,0.5),\n",
    "        6:(0.5,0.5,0),\n",
    "        7:(0.5,0.5,0.5),\n",
    "        8:(0.5,0.5,1),\n",
    "        9:(0.5,1,0.5),\n",
    "        10:(1,0.5,0.5)\n",
    "    }\n",
    "    return_images = True\n",
    "    image_size = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        # 1. Extract the tsplib95 file problem\n",
    "        # self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/A/A-n32-k5.vrp')\n",
    "#         self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/_singleTruck/A-n32-k5_3.vrp')\n",
    "        self.problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n53-k7.vrp')        \n",
    "        # 2. Create a networkx graph out of the problem. //will be plotting this\n",
    "        self.nx_graph = self.problem.get_graph()\n",
    "        self.edge_list = list(self.problem.get_edges()) #[(,)]\n",
    "        self.node_positions = self.problem.node_coords #dict\n",
    "        # the list of nodes\n",
    "        self.node_list = list(self.problem.get_nodes())\n",
    "        self.action_space = len(self.node_list) #the number of choices including staying put\n",
    "        # the depot location\n",
    "        self.depot_location = 1\n",
    "        # reseting the environment when initialized\n",
    "        self.reset_environment()\n",
    "        \n",
    "    def reset_environment(self):\n",
    "        # creating the Trucks\n",
    "        # 4. Extract the necessary data about the trucks. //no of trucks, depot_section, capacity\n",
    "        self.node_demands = copy.deepcopy(self.problem.demands)\n",
    "        truck_capacity = copy.deepcopy(self.problem.capacity)\n",
    "        self.truck = Truck(truck_capacity, 1, self.truck_colors.get(3))\n",
    "        self.episode_step = 0\n",
    "        self.total_hop_penalty = 0 #per episode\n",
    "        if self.return_images:\n",
    "            observation = np.array(self.get_image())\n",
    "        # there is no else case as we need always need the image for CNN\n",
    "        return observation\n",
    "    \n",
    "    # change the demand of the node when visited\n",
    "    def change_demand(self, node):\n",
    "        self.node_demands[node] = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        # moving the trucks for the action\n",
    "        self.truck.action(action)\n",
    "        self.truck.capacity -= self.node_demands.get(action)\n",
    "        self.change_demand(action)\n",
    "        # other truck actions\n",
    "\n",
    "        if self.return_images:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        # there is no else case as the return image is always true\n",
    "\n",
    "        # assinging the rewards and penalties\n",
    "        self.reward = 0\n",
    "        # checking if the demands have been satisfied\n",
    "        completed = False\n",
    "        if sum(list(self.node_demands.values())) == 0:\n",
    "            # print(\"***Satisfied***\")\n",
    "            # print(self.node_demands)\n",
    "            completed = True\n",
    "            self.reward = self.completion_reward\n",
    "        else:\n",
    "            # rewards for other trucks\n",
    "            self.node_penalty(self.truck) #other penalties\n",
    "            self.movement_penalty(self.truck) #edge weight\n",
    "            # penalties for other trucks\n",
    "        done = False\n",
    "        if self.reward == self.completion_reward:\n",
    "            #if self.reward == self.completion_reward or len(self.truck.path) >= self.permitted_path_length:\n",
    "            done = True\n",
    "            if sum(list(self.node_demands.values())) > 0:\n",
    "                self.reward -= self.failing_task_penalty\n",
    "\n",
    "        return new_observation, self.reward, done, completed\n",
    "\n",
    "    def node_penalty(self, truck):\n",
    "        if self.node_demands[truck.node] == 0:\n",
    "            if (truck.node == 1 and truck.capacity == truck.max_truck_capacity) or truck.node!=1:\n",
    "                self.reward -= self.zero_demand_penalty\n",
    "        else:\n",
    "            self.reward += self.demand_satisfying_reward\n",
    "\n",
    "        if self.truck.capacity <= 0:\n",
    "            self.reward -= self.non_positive_capacity_penalty\n",
    "\n",
    "    def movement_penalty(self, truck):\n",
    "        if truck.prev_node: #else it's 0\n",
    "            source_node = truck.prev_node\n",
    "            destination_node = truck.node\n",
    "            if source_node == destination_node: #if truck stays at the same place\n",
    "                self.reward -= self.hopping_incentive_penalty\n",
    "            self.reward -= self.problem.wfunc(source_node, destination_node)*10\n",
    "            self.total_hop_penalty += self.problem.wfunc(source_node, destination_node)\n",
    "            \n",
    "    def get_zero_demand_nodes(self, node_demands):\n",
    "        zero_demand_nodes_list = []\n",
    "        for key in node_demands:\n",
    "            if node_demands[key] == 0 and key != 1:\n",
    "                zero_demand_nodes_list.append(key)\n",
    "        return zero_demand_nodes_list\n",
    "    \n",
    "    def get_relative_demands(self, capacity, node_demands):\n",
    "        relative_node_demands = {}\n",
    "        for node in node_demands:\n",
    "            demand = node_demands[node]\n",
    "            relative_node_demands[node] = demand if capacity>=demand else 0\n",
    "        return relative_node_demands\n",
    "\n",
    "    def get_image(self):\n",
    "        # the initiated rgb image of the given size. \n",
    "        env = np.zeros((self.image_size, self.image_size, 3), dtype=np.uint8) #background is black\n",
    "        \n",
    "        relative_demand_values = self.get_relative_demands(self.truck.capacity, self.node_demands)\n",
    "        \n",
    "        for node in self.node_positions.keys():\n",
    "            node_coods = self.node_positions.get(node)\n",
    "            env[int(node_coods[0])][int(node_coods[1])] = (0,255,0) #nodes are green\n",
    "        \n",
    "        node_coods = self.node_positions.get(1)\n",
    "        env[int(node_coods[0])][int(node_coods[1])] = (0,0,0) #depot is black\n",
    "        \n",
    "        if self.truck.path: #if there are elements in the path\n",
    "            for visited_node in set(self.truck.path):\n",
    "                node_coods = self.node_positions.get(visited_node)\n",
    "                env[int(node_coods[0])][int(node_coods[1])] = (255,255,255) #visited nodes are white\n",
    "        #zero_demand_nodes = self.get_zero_demand_nodes(self.node_demands)\n",
    "        zero_demand_nodes = self.get_zero_demand_nodes(relative_demand_values)\n",
    "        if zero_demand_nodes:\n",
    "            for node in zero_demand_nodes:\n",
    "                node_coods = self.node_positions.get(node)\n",
    "                env[int(node_coods[0])][int(node_coods[1])] = (255,0,0) #danger are red\n",
    "        \n",
    "        node_coods = self.node_positions.get(self.truck.node)\n",
    "        env[int(node_coods[0])][int(node_coods[1])] = (255,255,0) #truck position\n",
    "        \n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        # trying to reduce to size to decrease the time taken\n",
    "        #img = img.resize((10,10))\n",
    "        return img\n",
    "    \n",
    "#     def render(self):\n",
    "#         img = self.get_image()\n",
    "#         img = img.resize((300,300))\n",
    "#         cv2.imshow(\"image\", np.array(img))\n",
    "#         cv2.waitKey(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjqrxI5rr3s_"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # main model\n",
    "        self.main_model = self.create_model()\n",
    "        \n",
    "        # target model\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "        # an array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=considering_training_length)\n",
    "        # self.replay_memory = [] #removing the limit\n",
    "\n",
    "        # used to know when to update target n/w with main n/w's weights\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # model.add(Conv2D(256, (10,10), input_shape=environment.observation_space))\n",
    "        model.add(Conv2D(256, (2,2), input_shape=environment.observation_space))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # model.add(Conv2D(256,(10,10)))\n",
    "        model.add(Conv2D(256,(3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # converting the 3D features into the 1D feature\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(environment.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # adding the step data into the array to be considered later\n",
    "    def update_replay_memory(self, step):\n",
    "        self.replay_memory.append(step)\n",
    " \n",
    "    def train(self, terminal_state, step):\n",
    "        # start training only when we have a certain number of samples already saved\n",
    "        if len(self.replay_memory)< min_replay_memory_size:\n",
    "            return\n",
    "        # get the minibatch of the samples from the replay table - OLD\n",
    "        minibatch = random.sample(self.replay_memory, min_training_length)\n",
    "        # get the minibatch of the samples that have the highest reward\n",
    "        #minibatch = self.get_max_rewarded_steps(self.replay_memory, min_training_length)\n",
    "        \n",
    "        # get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.main_model.predict(current_states)\n",
    "        #print(current_qs_list)\n",
    "\n",
    "        # get future states from the minibatch, then query NN model for Q values\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # enumerating through the batches\n",
    "        for index,(current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            # if its not a terminal state, get new q from future states or else set to 0\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + discount*max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # update the q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            #print(len(current_qs))\n",
    "            #-1 because indexing 0-4 and actions from 1-5\n",
    "            current_qs[action-1] = new_q\n",
    "            \n",
    "\n",
    "            # append to the training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "        \n",
    "        # fit on all samples as one batch\n",
    "        self.main_model.fit(np.array(X)/255, np.array(y), batch_size=min_training_length, verbose=0, shuffle=False, callbacks=[LearningRateReducerCb()])\n",
    "        \n",
    "        # updating the target n/w counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # if the counter reaches the required value...\n",
    "        # update the target n/w with weights of main n/w\n",
    "        if self.target_update_counter > update_target_every:\n",
    "            self.target_model.set_weights(self.main_model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "    \n",
    "    # query the main n/w for q values given the current observation space\n",
    "    def get_qs(self, state):\n",
    "        return self.main_model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    def get_models(self):\n",
    "        return self.main_model, self.target_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IW4408ZUtK-"
   },
   "outputs": [],
   "source": [
    "# model related constants\n",
    "discount = 0.9 #the discount applied to the DQN equation\n",
    "# the environment keeps running till the demand of the nodes is satisfied\n",
    "considering_training_length = 50_000 #the no of steps considered for training\n",
    "min_training_length = 100 #the no of steps used for training\n",
    "update_target_every = 5 #terminal states (end of episodes)\n",
    "min_replay_memory_size = 1000 #min no of steps in a memory to start training\n",
    "\n",
    "environment = VRPEnvironment()\n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bb223c95saFs",
    "outputId": "4c0b24b1-e1f3-4b53-eba2-538d4ea124de"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8dd902f314ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart_decaying\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;31m#making this True ignores the conditional decaying - Notes 2.a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_of_episodes\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mepsilon_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrewards_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "show_preview = False #using this will make it slow. Don't care about this now.\n",
    "aggregrate_stats_every = 50\n",
    "start_decaying = True #making this True ignores the conditional decaying - Notes 2.a\n",
    "\n",
    "x_axis = list(range(environment.no_of_episodes+1))\n",
    "epsilon_list = []\n",
    "rewards_list = []\n",
    "path_length_list = []\n",
    "path_taken_list = []\n",
    "hop_penalty_list = []\n",
    "\n",
    "epsilon_decaying_factor = 3000\n",
    "\n",
    "plot_rep = 1000\n",
    "\n",
    "# iterate over the episodes\n",
    "for episode in tqdm(range(1, environment.no_of_episodes + 1), ascii=True, unit='episodes'):\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    # Reset environment and get initial state\n",
    "    current_state = environment.reset_environment()\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > environment.epsilon:\n",
    "            # Get action from Q table            \n",
    "            action = np.argmax(agent.get_qs(current_state))+1\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(1, environment.action_space+1)\n",
    "\n",
    "        new_state, reward, done, completed = environment.step(action)\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # no rendering\n",
    "#         if show_preview and not episode % aggregrate_stats_every:\n",
    "#             environment.render()\n",
    "        \n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "        # Decay the Epsilon\n",
    "#         if completed:\n",
    "#             start_decaying = True\n",
    "        \n",
    "#         if start_decaying:\n",
    "        if environment.epsilon > environment.min_epsilon:\n",
    "            environment.epsilon = np.exp(-episode/epsilon_decaying_factor)\n",
    "            environment.epsilon = max(environment.min_epsilon, environment.epsilon)\n",
    "        \n",
    "        #print(environment.truck.path)\n",
    "        \n",
    "    epsilon_list.append(environment.epsilon*1000)\n",
    "    rewards_list.append(episode_reward)\n",
    "    path_length_list.append(len(environment.truck.path)*100)\n",
    "    path_taken_list.append(environment.truck.path)\n",
    "    hop_penalty_list.append(environment.total_hop_penalty)\n",
    "    \n",
    "    print(\"Path is:\", end=\" \")\n",
    "    print(environment.truck.path, end=\";\")\n",
    "    print(\"P and R: \"+str(episode_reward), end=\";\")\n",
    "    print(\"Capacity: \"+str(environment.truck.capacity), end=\";\")\n",
    "    print(\"Epsilon: \"+str(environment.epsilon), end=\";\")\n",
    "    print(\"Hop Penalty: \"+str(environment.total_hop_penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bX2d49yTUtKz",
    "outputId": "bb499844-3e52-40a8-bcd7-da50012245d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder is already there\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.mkdir('drive/saved_plots')\n",
    "except:\n",
    "    print(\"folder is already there\")\n",
    "    \n",
    "def plot_this(epsilon_list, rewards_list, path_length_list, environment, hop_penalty_list):\n",
    "    x_axis = list(range(environment.no_of_episodes))\n",
    "    x_axis = x_axis[:len(path_length_list)]\n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    x_axis = x_axis[:len(epsilon_list)]\n",
    "    hp = [i*100 for i in hop_penalty_list]\n",
    "    hop_penalty_list = hp\n",
    "    \n",
    "    plt.plot(x_axis,epsilon_list,'r', label='Epsilon (scaled by 1000)')\n",
    "    plt.plot(x_axis,rewards_list,'g', label='Episode Rewards')\n",
    "    plt.plot(x_axis,hop_penalty_list, 'k', label='Hop Penalty (scaled by 1000)',linestyle=\"\",marker=\"o\", markersize=0.75)\n",
    "    plt.plot(x_axis,path_length_list,'b', label='Path Length (scaled by 100)')\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel('->Episodes')\n",
    "    plt.ylabel('->Epsilon*1000, Rewards, Path Length*100')\n",
    "    plt.text(17600, 18000, f'Epsilon Decaying Factor: {epsilon_decaying_factor}')\n",
    "    plt.text(17600, 16000, f'Adam learning rate: 0.001')\n",
    "    plt.title('Epsilon, Rewards, Path Length vs Episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3e050813352d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "print(environment.truck.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWLdEBhZLRuR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilon_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7af622e04264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_this\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_length_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_penalty_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'saved_plots/plot_{epsilon_decaying_factor}_min_eps_{environment.min_epsilon}_lr_0.001_discount_{discount}_hop_penaltyx10.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epsilon_list' is not defined"
     ]
    }
   ],
   "source": [
    "plot_this(epsilon_list, rewards_list, path_length_list, environment, hop_penalty_list)\n",
    "plt.savefig(f'saved_plots/plot_{epsilon_decaying_factor}_min_eps_{environment.min_epsilon}_lr_0.001_discount_{discount}_hop_penaltyx10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "IASPWuPoUtMc",
    "outputId": "36510e70-113d-4b04-c177-14965a9e3f9b"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-1d974bf023c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msample_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# print(path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msample_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-1d974bf023c2>\u001b[0m in \u001b[0;36mget_paths\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msample_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msample_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msample_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'pop'"
     ]
    }
   ],
   "source": [
    "def get_paths(path):\n",
    "    sample_path = []\n",
    "    sample_path.append(path.pop(0))\n",
    "    sample_path.append(random.sample(path,3))\n",
    "    sample_path.append(path.pop(-1))\n",
    "    return sample_path\n",
    "# print(path)\n",
    "sample_paths = get_paths(path)\n",
    "for i in sample_paths:\n",
    "    plt.figure()\n",
    "    plt.plot(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnuDH5DlUtNl",
    "outputId": "59d9e444-acdf-4867-d0c2-01984273f2bd"
   },
   "outputs": [],
   "source": [
    "# # Save the entire model as a SavedModel.\n",
    "# import os\n",
    "# # !mkdir -p saved_model\n",
    "# try:\n",
    "#     os.mkdir('saved_models')\n",
    "# except:\n",
    "#     print('already there')\n",
    "# # mm, tm = DQNAgent.get_models()\n",
    "# agent.main_model.save(f'saved_models/main_model_ef_{epsilon_decaying_factor}_mineps_{environment.min_epsilon}_lr_0.01.h5_hop_penaltyx10')\n",
    "# agent.target_model.save(f'saved_models/target_model_ef_{epsilon_decaying_factor}_mineps_{environment.min_epsilon}_lr_0.01.h5_hop_penaltyx10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnFnTVnEUtNp",
    "outputId": "d30b06e6-85d0-47e8-ccc7-1c7677456b2f"
   },
   "outputs": [],
   "source": [
    "# plot_this(epsilon_scaled, rewards_list, path_scaled, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1u2PzUxLYsO",
    "outputId": "31b1713d-d0dd-4ebb-ac96-1dca98ade3a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2202c6a3b38>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = list(range(25000))\n",
    "# print(x)\n",
    "k = 3000\n",
    "y = [float(np.exp(-i/k)) for i in x]\n",
    "# print(y)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjSRWi8kUtNz",
    "outputId": "0a79583a-08e1-469c-dbdf-7c8a0b3a92de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (3, 2), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "class keka:\n",
    "    def test(self,i):\n",
    "        return i[1]\n",
    "    a = [(1,3),(2,1),(3,2)]\n",
    "    def uppu(self):\n",
    "        self.a.sort(reverse=True,key=self.test)\n",
    "        return self.a\n",
    "    \n",
    "k = keka()\n",
    "print(k.uppu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccnRvY5MUtN2",
    "outputId": "96b8958e-91ab-44a5-f9d1-688f2dbc0ce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n32-k5_4.vrp')\n",
    "len(list(problem.get_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tvWna_EUtN6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vrp_dqn_update_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
