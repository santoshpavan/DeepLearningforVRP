{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "0R6chiOssBaj",
    "outputId": "42306fe1-4642-4702-a61f-1efb41333fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"vrp_dqn.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1iothquIlGtKKte5KIxO-YCKzXnZGjbK-\n",
    "\"\"\"\n",
    "# !pip install tsplib95\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import tsplib95\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.python.keras import Sequential\n",
    "# from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "import io # to save the image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZA_bOlXdsHOh",
    "outputId": "095084ca-9e58-4121-e0cc-117ee20db7ca"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9iofX44xw0s"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2\n",
    "class LearningRateReducerCb(tensorflow.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # old_lr = self.model.optimizer.lr.read_value()\n",
    "        # new_lr = old_lr * 0.99\n",
    "        new_lr = 0.1\n",
    "        # print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "        self.model.optimizer.lr.assign(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8NT6EoJDsQ4"
   },
   "outputs": [],
   "source": [
    "# 6. Define the movement restriction of the truck.\n",
    "class Truck:\n",
    "    def __init__(self, capacity, id, color):\n",
    "        self.id = id\n",
    "        self.color = color\n",
    "        self.path = [] #this has the list of nodes it has visited\n",
    "        self.max_truck_capacity = copy.deepcopy(capacity) #the max capacity\n",
    "        self.capacity = copy.deepcopy(capacity)\n",
    "        #self.visit_depo()\n",
    "        self.prev_node = None\n",
    "        self.node = 1 #starts from the depo\n",
    "\n",
    "    def action(self, choice):\n",
    "        # the number of choice of actions are the number of nodes-1\n",
    "        # the choice to be taken depends on the demands - penalty based\n",
    "        # the choice number is the same as the node number\n",
    "        # it is not a choice if the demand is 0 - changing this to penalty\n",
    "        #!! Want the system to learn instead\n",
    "        # if self.capacity == 0:\n",
    "        #     self.visit_depo()\n",
    "        self.move(choice)\n",
    "\n",
    "    def move(self, to_node_value):\n",
    "        # node_list_copy = copy.deepcopy(node_list)\n",
    "        # node_list_copy.remove(1)\n",
    "        # select a random node to go to\n",
    "        #if not to_node_value: #to_node_value is False by default\n",
    "        #    to_node_value = random.choice(self.node_list)\n",
    "        if to_node_value == 1:\n",
    "            self.visit_depo()\n",
    "        self.prev_node = self.node\n",
    "        self.node = to_node_value\n",
    "        self.path.append(to_node_value)\n",
    "        # when invoked update the demand of the node\n",
    "        # update the demand of the node\n",
    "\n",
    "    def visit_depo(self):\n",
    "        self.prev_node = self.node\n",
    "        self.node = 1 #here it is 1\n",
    "        self.capacity = copy.deepcopy(self.max_truck_capacity) #truck capacity reset\n",
    "        self.path.append(1)\n",
    "    \n",
    "    #def path(self, node_value):\n",
    "    #    self.path.append(node_value)\n",
    "\n",
    "    # def get_node(self):\n",
    "    #     return self.node\n",
    "\n",
    "    \n",
    "    # def get_capacity(self):\n",
    "    #     return self.capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjYDwT6rD15Q"
   },
   "outputs": [],
   "source": [
    "class VRPEnvironment:\n",
    "    # environment related constants\n",
    "    #https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.figure.html\n",
    "    #observation_space = (640,480,3)\n",
    "    # observation_space = (100,100,3)\n",
    "    observation_space = (10,10,3)\n",
    "    # penalty and rewards\n",
    "    non_positive_capacity_penalty = 100\n",
    "    zero_demand_penalty = 100 #truck goes to a zero demand node - except 1\n",
    "    # ignore the failing task penalty as the episode will end only when...\n",
    "    # all the demands are satisfied i.e. task will always be success...\n",
    "    # The penalty of the achievement needs to go down.\n",
    "    failing_task_penalty = 100 #trucks fail to complete the task\n",
    "    completion_reward = 5000 #trucks complete the task\n",
    "    demand_satisfying_reward = 100 #another incentive to hit the right target. Imp with considering each step\n",
    "    hopping_incentive_penalty = 500 #staying at the same node\n",
    "    # visit_correct_node_reward = 100\n",
    "    # exploration settings\n",
    "    permitted_path_length = 20 #30\n",
    "    epsilon = 1\n",
    "    epsilon_decay = 0.999#changing this from 0.999\n",
    "    min_epsilon = 0.001 #0.001\n",
    "    no_of_episodes = 10_000 #30_000\n",
    "    # from 0 to 0.5 difference is small so using 1 first\n",
    "    truck_colors = {\n",
    "        1:(0,0,1),\n",
    "        2:(0,1,0),\n",
    "        3:(1,0,0),\n",
    "        4:(0,0.5,0.5),\n",
    "        5:(0.5,0,0.5),\n",
    "        6:(0.5,0.5,0),\n",
    "        7:(0.5,0.5,0.5),\n",
    "        8:(0.5,0.5,1),\n",
    "        9:(0.5,1,0.5),\n",
    "        10:(1,0.5,0.5)\n",
    "    }\n",
    "    return_images = True\n",
    "    image_size = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        # 1. Extract the tsplib95 file problem\n",
    "        # self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/A/A-n32-k5.vrp')\n",
    "        # self.problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/_singleTruck/A-n32-k5_2.vrp')\n",
    "        self.problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n32-k5_3.vrp')        \n",
    "        # 2. Create a networkx graph out of the problem. //will be plotting this\n",
    "        self.nx_graph = self.problem.get_graph()\n",
    "        self.edge_list = list(self.problem.get_edges()) #[(,)]\n",
    "        self.node_positions = self.problem.node_coords #dict\n",
    "        # the list of nodes\n",
    "        self.node_list = list(self.problem.get_nodes())\n",
    "        self.action_space = len(self.node_list) #the number of choices including staying put\n",
    "        # the depot location\n",
    "        self.depot_location = 1\n",
    "        # assigning the default color of the nodes to be black\n",
    "        # node_color_values = [(0,0,0) for i in range(len(node_list))]\n",
    "        # reseting the environment when initialized\n",
    "        self.reset_environment()\n",
    "        \n",
    "    def reset_environment(self):\n",
    "        # creating the Trucks\n",
    "        # 4. Extract the necessary data about the trucks. //no of trucks, depot_section, capacity\n",
    "        self.node_demands = copy.deepcopy(self.problem.demands)\n",
    "        truck_capacity = copy.deepcopy(self.problem.capacity)\n",
    "        # trying hardcoding for now\n",
    "        self.truck = Truck(truck_capacity, 1, self.truck_colors.get(3))\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.return_images:\n",
    "            observation = np.array(self.get_image())\n",
    "        \n",
    "        # resetting the environment reward value\n",
    "        # self.reward = 0\n",
    "        # there is no else case as we need always need the image for CNN\n",
    "        return observation\n",
    "\n",
    "    # def calculate_move_penalty(self, source_node, dest_node):\n",
    "    #     return self.problem.wfunc(source_node, dest_node) #the weight of the edge\n",
    "\n",
    "    # change the demand of the node when visited\n",
    "    def change_demand(self, node):\n",
    "        self.node_demands[node] = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        # moving the trucks for the action\n",
    "        self.truck.action(action)\n",
    "        #print(\"action=\"+str(action)+\"\\n++++++\")\n",
    "        self.truck.capacity -= self.node_demands.get(action)\n",
    "        self.change_demand(action)\n",
    "        # print(self.truck.capacity)\n",
    "        # other truck actions\n",
    "\n",
    "        if self.return_images:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        # there is no else case as the return image is always true\n",
    "\n",
    "        # assinging the rewards and penalties\n",
    "        self.reward = 0\n",
    "        # checking if the demands have been satisfied\n",
    "        completed = False\n",
    "        if sum(list(self.node_demands.values())) == 0:\n",
    "            print(\"\\n***Satisfied***\")\n",
    "            # print(self.node_demands)\n",
    "            completed = True\n",
    "            self.reward = self.completion_reward\n",
    "        else:\n",
    "            # rewards for other trucks\n",
    "            # print(\"before\"+str(self.reward))\n",
    "            self.node_penalty(self.truck) #other penalties\n",
    "            self.movement_penalty(self.truck) #edge weight\n",
    "            # print(\"after\"+str(self.reward))\n",
    "            # penalties for other trucks\n",
    "        done = False\n",
    "        if self.reward == self.completion_reward or len(self.truck.path) >= self.permitted_path_length:\n",
    "        # if self.reward == self.completion_reward:\n",
    "            done = True\n",
    "            if sum(list(self.node_demands.values())) > 0:\n",
    "                self.reward -= self.failing_task_penalty\n",
    "\n",
    "        return new_observation, self.reward, done, completed\n",
    "\n",
    "    def node_penalty(self, truck):\n",
    "        if self.node_demands[truck.node] == 0:\n",
    "            if (truck.node == 1 and truck.capacity == truck.max_truck_capacity) or truck.node!=1:\n",
    "                self.reward -= self.zero_demand_penalty\n",
    "        else:\n",
    "            self.reward += self.demand_satisfying_reward\n",
    "\n",
    "        if self.truck.capacity <= 0:\n",
    "            self.reward -= self.non_positive_capacity_penalty\n",
    "        # return self.reward\n",
    "\n",
    "    def movement_penalty(self, truck):\n",
    "        #print(self.truck.path)\n",
    "        #print(\"-------\")\n",
    "        if truck.prev_node: #else it's 0\n",
    "            source_node = truck.prev_node\n",
    "            destination_node = truck.node\n",
    "            if source_node == destination_node: #if truck stays at the same place\n",
    "                self.reward -= self.hopping_incentive_penalty\n",
    "            self.reward -= self.problem.wfunc(source_node, destination_node)\n",
    "            # return self.problem.wfunc(source_node, destination_node)\n",
    "        # else:\n",
    "        #     source_node = truck.node\n",
    "        #     destination_node = truck.node\n",
    "        #     return 0\n",
    "    # \"\"\"\n",
    "    def get_image(self):\n",
    "        # the initiated rgb image of the given size. image_size = 100\n",
    "        env = np.zeros((self.image_size, self.image_size, 3), dtype=np.uint8)\n",
    "        for node in self.node_positions.keys():\n",
    "            node_coods = self.node_positions.get(node)\n",
    "            env[int(node_coods[0])][int(node_coods[1])] = (255,255,255)\n",
    "        if self.truck.path: #if there are elements in the path\n",
    "            for visited_node in set(self.truck.path):\n",
    "                node_coods = self.node_positions.get(visited_node)\n",
    "                #print(node_coods)\n",
    "                #print(visited_node)\n",
    "                #print(self.truck.path)\n",
    "                #print(len(env))\n",
    "                #print(\"--------\")\n",
    "                #try: #HAVING 0 HERE WAS CAUSING ERRORS! IGNORING 0 FOR NOW- STUPID\n",
    "                env[int(node_coods[0])][int(node_coods[1])] = (255,0,0)\n",
    "                #except:\n",
    "                    #print(node_coods)\n",
    "                    #print(visited_node)\n",
    "                    #print(self.truck.path)\n",
    "                    #print(len(env))\n",
    "                    #print(\"--------\")\n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        # trying to reduce to size to decrease the time taken\n",
    "        img = img.resize((10,10))\n",
    "        return img\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((500,500))\n",
    "        cv2.imshow(\"image\", np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "    \"\"\"\n",
    "    def get_image(self):\n",
    "        fig,ax = plt.subplots()\n",
    "        node_color_values = [(0,0,1) for i in range(len(self.node_list))]\n",
    "        # ax.clear()\n",
    "        if not self.truck.path: #if it is not empty\n",
    "            for i in self.truck.path:\n",
    "                node_color_values[i] = (1,0,0)\n",
    "        image_io = io.BytesIO()\n",
    "        #edge color is white to ignore it\n",
    "        nx.draw(self.nx_graph, pos=self.node_positions, with_labels=False, node_color=node_color_values, node_size=20, edge_color=(1,1,1))\n",
    "        fig.savefig(image_io, dpi=5)\n",
    "        image = Image.open(image_io)\n",
    "        image = image.resize((10,10))\n",
    "        image.close() #keeping it open consumes a lot of memory\n",
    "        return image\n",
    "\n",
    "    def render(self):\n",
    "        image = self.get_image()\n",
    "        image = image.resize((1000,1000))\n",
    "        cv2.imshow(\"image\",np.array(image))\n",
    "        cv2.waitKey(1)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjqrxI5rr3s_"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # main model\n",
    "        self.main_model = self.create_model()\n",
    "        \n",
    "        # target model\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "        # an array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=considering_training_length)\n",
    "\n",
    "        # used to know when to update target n/w with main n/w's weights\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # model.add(Conv2D(256, (10,10), input_shape=environment.observation_space))\n",
    "        model.add(Conv2D(256, (3,3), input_shape=environment.observation_space))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # model.add(Conv2D(256,(10,10)))\n",
    "        model.add(Conv2D(256,(3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # converting the 3D features into the 1D feature\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "\n",
    "        model.add(Dense(environment.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # adding the step data into the array to be considered later\n",
    "    def update_replay_memory(self, step):\n",
    "        #print(step[2])\n",
    "        #print(\"****\")\n",
    "        self.replay_memory.append(step)\n",
    "        \n",
    "    def consider_reward(self, element):\n",
    "        return element[1]\n",
    "        \n",
    "    def get_max_rewarded_steps(self, replay_memory, length):\n",
    "        #replay_memory has the steps and steps[2] is the rewards\n",
    "        # max = -math.inf\n",
    "        step_reward_pairs = []\n",
    "        for step in replay_memory:\n",
    "            if len(step_reward_pairs) <= length:\n",
    "                step_reward_pairs.append((step, step[2]))\n",
    "            else:\n",
    "                step_reward_pairs.sort(reverse=True, key=self.consider_reward)\n",
    "                if step_reward_pairs[-1][1] < step[2]:\n",
    "                    step_reward_pairs.pop() #removing the least value i.e. the last value\n",
    "                    step_reward_pairs.append((step, step[2]))\n",
    "        steps_list = []\n",
    "        for step_reward_pair in step_reward_pairs:\n",
    "            steps_list.append(step_reward_pair[0])\n",
    "        return steps_list\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "        # start training only when we have a certain number of samples already saved\n",
    "        if len(self.replay_memory)< min_replay_memory_size:\n",
    "            return\n",
    "        # get the minibatch of the samples from the replay table - OLD\n",
    "        #minibatch = random.sample(self.replay_memory, min_training_length)\n",
    "        # get the minibatch of the samples that have the highest reward\n",
    "        minibatch = self.get_max_rewarded_steps(self.replay_memory, min_training_length)\n",
    "        \n",
    "        # get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.main_model.predict(current_states)\n",
    "\n",
    "        # get future states from the minibatch, then query NN model for Q values\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        # enumerating through the batches\n",
    "        for index,(current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            # if its not a terminal state, get new q from future states or else set to 0\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + discount*max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # update the q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # append to the training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "        \n",
    "        # fit on all samples as one batch\n",
    "        self.main_model.fit(np.array(X)/255, np.array(y), batch_size=min_training_length, verbose=0, shuffle=False, callbacks=[LearningRateReducerCb()])\n",
    "        \n",
    "        # updating the target n/w counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # if the counter reaches the required value...\n",
    "        # update the target n/w with weights of main n/w\n",
    "        if self.target_update_counter > update_target_every:\n",
    "            self.target_model.set_weights(self.main_model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "    \n",
    "    # query the main n/w for q values given the current observation space\n",
    "    def get_qs(self, state):\n",
    "        return self.main_model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model related constants\n",
    "discount = 0.99 #the discount applied to the DQN equation\n",
    "# the environment keeps running till the demand of the nodes is satisfied\n",
    "considering_training_length = 50_000 #the no of steps considered for training\n",
    "min_training_length = 100 #the no of steps used for training\n",
    "# episodes = 30_000\n",
    "update_target_every = 5 #terminal states (end of episodes)\n",
    "min_replay_memory_size = 1000 #min no of steps in a memory to start training\n",
    "\n",
    "environment = VRPEnvironment()\n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bb223c95saFs",
    "outputId": "f5525649-01fb-4d00-8df6-0a5ffe528010"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                             | 0/10000 [00:00<?, ?episodes/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|1                                                                                                                                                                    | 9/10000 [00:00<03:09, 52.69episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 8, 6, 1, 1, 2, 7, 4, 1, 1, 2, 9];P and R: 3074;Capacity: 99;Epsilon: 0.9993335555061811\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [9, 2, 1, 1, 2, 4, 2, 8, 1, 1, 7, 6, 7, 3, 5];P and R: 2749;Capacity: 50;Epsilon: 0.9986675551606254\n",
      "Path is: [4, 8, 5, 3, 8, 8, 8, 4, 2, 5, 2, 5, 7, 3, 8, 4, 5, 1, 1, 3];P and R: -3571;Capacity: 100;Epsilon: 0.9980019986673331\n",
      "Path is: [7, 7, 1, 1, 6, 8, 7, 7, 7, 7, 9, 9, 2, 3, 7, 6, 5, 6, 9, 9];P and R: -5544;Capacity: 35;Epsilon: 0.9973368857305009\n",
      "Path is: [8, 1, 1, 5, 4, 6, 6, 4, 7, 6, 7, 8, 3, 3, 9, 4, 6, 9, 9, 1, 1];P and R: -4558;Capacity: 100;Epsilon: 0.9966722160545233\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 9, 3, 2, 4, 2, 5, 7, 5, 5, 4, 8, 6];P and R: 3258;Capacity: 2;Epsilon: 0.9960079893439915\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [9, 5, 6, 2, 4, 7, 9, 8];P and R: 4268;Capacity: 2;Epsilon: 0.9953442053036935\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 3, 4, 9, 4, 2, 3, 4, 1, 1, 5, 1, 1, 8, 7, 2, 9, 6];P and R: 2435;Capacity: 58;Epsilon: 0.9946808636386143\n",
      "Path is: [1, 1, 3, 2, 1, 1, 8, 6, 3, 2, 2, 3, 1, 1, 7, 1, 1, 3, 1, 1];P and R: -4634;Capacity: 100;Epsilon: 0.9940179640539353\n",
      "Path is: [7, 4, 8, 5, 1, 1, 5, 7, 4, 9, 5, 9, 9, 3, 7, 6, 1, 1, 5, 4];P and R: -3473;Capacity: 100;Epsilon: 0.9933555062550344\n",
      "Path is: [9, 1, 1, 3, 9, 8, 3, 6, 4, 1, 1, 7, 1, 1, 4, 3, 9, 6, 7, 3];P and R: -3369;Capacity: 100;Epsilon: 0.9926934899474861\n",
      "\n",
      "***Satisfied***"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|3                                                                                                                                                                   | 21/10000 [00:00<02:38, 62.91episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path is: [5, 6, 6, 4, 7, 3, 1, 1, 6, 9, 8, 4, 7, 2];P and R: 2758;Capacity: 73;Epsilon: 0.9920319148370607\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 6, 5, 4, 1, 1, 7, 2, 6, 2, 8, 9];P and R: 3450;Capacity: 79;Epsilon: 0.9913707806297248\n",
      "Path is: [6, 7, 5, 8, 8, 7, 1, 1, 6, 3, 4, 6, 4, 9, 1, 1, 8, 4, 6, 4];P and R: -3462;Capacity: 100;Epsilon: 0.9907100870316412\n",
      "Path is: [3, 2, 3, 3, 3, 1, 1, 9, 8, 3, 3, 5, 9, 6, 3, 4, 2, 5, 5, 3];P and R: -4551;Capacity: 22;Epsilon: 0.9900498337491681\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 3, 2, 1, 1, 9, 5, 2, 4, 4, 7, 8, 7, 9, 6];P and R: 2165;Capacity: 10;Epsilon: 0.9893900204888596\n",
      "Path is: [5, 3, 1, 1, 8, 7, 3, 7, 2, 3, 5, 8, 3, 5, 8, 4, 9, 3, 4, 3];P and R: -2583;Capacity: 40;Epsilon: 0.9887306469574654\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [3, 8, 6, 7, 8, 6, 4, 5, 7, 4, 1, 1, 8, 5, 1, 1, 1, 1, 9, 2];P and R: 1846;Capacity: 91;Epsilon: 0.9880717128619305\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 2, 8, 9, 3, 2, 6, 2, 2, 3, 7, 4, 6, 8, 9, 7, 1, 1, 2, 5];P and R: 2131;Capacity: 86;Epsilon: 0.9874132179093955\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 9, 1, 1, 6, 4, 5, 2, 4, 2, 7];P and R: 3568;Capacity: 21;Epsilon: 0.9867551618071957\n",
      "Path is: [1, 1, 4, 2, 2, 8, 2, 8, 5, 5, 3, 3, 9, 3, 8, 5, 4, 9, 6, 6];P and R: -4576;Capacity: 14;Epsilon: 0.9860975442628619\n",
      "Path is: [9, 6, 4, 9, 7, 8, 3, 8, 7, 9, 8, 1, 1, 3, 6, 7, 5, 6, 5, 1, 1];P and R: -3058;Capacity: 100;Epsilon: 0.9854403649841196\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 1, 1, 8, 6, 3, 5, 4, 2, 6, 7, 1, 1, 4, 9];P and R: 2755;Capacity: 99;Epsilon: 0.9847836236788889\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 9, 1, 1, 1, 1, 7, 5, 3, 6, 3, 9, 6, 4, 4, 8];P and R: 2153;Capacity: 11;Epsilon: 0.9841273200552851\n",
      "Path is: [4, 7, 1, 1, 1, 1, 5, 2, 7, 7, 4, 7, 3, 5, 1, 1, 2, 6, 3, 8];P and R: -3860;Capacity: 58;Epsilon: 0.9834714538216175\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 3, 4, 1, 1, 8, 9, 2, 4, 3, 1, 1, 1, 1, 7, 1, 1, 6, 5];P and R: 1546;Capacity: 62;Epsilon: 0.98281602468639\n",
      "Path is: [8, 9, 4, 2, 1, 1, 8, 2, 4, 2, 2, 3, 5, 9, 4, 7, 8, 7, 2, 7];P and R: -3079;Capacity: 74;Epsilon: 0.9821610323583008\n",
      "Path is: [6, 4, 6, 1, 1, 3, 2, 4, 8, 8, 8, 2, 4, 9, 3, 1, 1, 3, 4, 8];P and R: -3961;Capacity: 100;Epsilon: 0.981506476546242\n",
      "Path is: [5, 3, 1, 1, 2, 4, 7, 5, 1, 1, 8, 6, 7, 1, 1, 4, 8, 1, 1, 7];P and R: -3752;Capacity: 100;Epsilon: 0.9808523569593002\n",
      "Path is: [1, 1, 7, 4, 7, 5, 1, 1, 6, 3, 7, 2, 1, 1, 9, 6, 3, 7, 2, 3];P and R: -3376;Capacity: 99;Epsilon: 0.9801986733067553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|5                                                                                                                                                                   | 31/10000 [00:00<02:28, 67.10episodes/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|6                                                                                                                                                                   | 37/10000 [00:00<02:39, 62.33episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 6, 7, 4, 2, 9, 1, 1, 5, 7, 8];P and R: 3567;Capacity: 82;Epsilon: 0.9795454252980814\n",
      "Path is: [7, 8, 5, 8, 1, 1, 3, 6, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1];P and R: -4531;Capacity: 100;Epsilon: 0.9788926126429459\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [8, 3, 6, 7, 6, 8, 6, 7, 8, 5, 3, 3, 3, 2, 8, 4, 5, 6, 9];P and R: 2145;Capacity: 2;Epsilon: 0.97824023505121\n",
      "Path is: [8, 4, 9, 1, 1, 3, 4, 9, 9, 5, 2, 6, 4, 6, 2, 3, 8, 9, 6, 5];P and R: -3073;Capacity: 54;Epsilon: 0.977588292232928\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 7, 9, 2, 3, 4, 9, 6, 9, 9, 8, 3, 6, 2, 5];P and R: 3047;Capacity: 2;Epsilon: 0.9769367838983476\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [9, 2, 5, 4, 8, 1, 1, 1, 1, 2, 1, 1, 9, 2, 6, 2, 3, 7];P and R: 2035;Capacity: 64;Epsilon: 0.9762857097579093\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 9, 9, 4, 1, 1, 7, 5, 9, 2, 9, 9, 3, 3, 7, 6, 9, 7, 8];P and R: 728;Capacity: 24;Epsilon: 0.9756350695222471\n",
      "Path is: [4, 6, 8, 9, 7, 1, 1, 4, 6, 1, 1, 6, 9, 8, 2, 6, 3, 8, 8, 2];P and R: -3465;Capacity: 92;Epsilon: 0.9749848629021873\n",
      "Path is: [1, 1, 1, 1, 4, 3, 3, 1, 1, 3, 4, 9, 6, 1, 1, 6, 7, 1, 1, 8];P and R: -4637;Capacity: 82;Epsilon: 0.9743350896087494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|7                                                                                                                                                                   | 43/10000 [00:00<03:07, 53.07episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is: [3, 2, 9, 8, 3, 1, 1, 5, 2, 1, 1, 6, 2, 1, 1, 9, 6, 1, 1, 5];P and R: -3765;Capacity: 100;Epsilon: 0.973685749353145\n",
      "Path is: [2, 4, 5, 8, 2, 9, 5, 8, 2, 4, 9, 2, 2, 4, 2, 8, 5, 3, 9, 5];P and R: -2695;Capacity: 38;Epsilon: 0.9730368418467786\n",
      "Path is: [9, 1, 1, 5, 9, 8, 1, 1, 8, 6, 1, 1, 7, 3, 8, 1, 1, 5, 5, 2];P and R: -4267;Capacity: 92;Epsilon: 0.9723883668012469\n",
      "Path is: [7, 5, 2, 8, 7, 2, 4, 5, 7, 7, 2, 3, 9, 1, 1, 4, 9, 1, 1, 5];P and R: -3470;Capacity: 100;Epsilon: 0.9717403239283386\n",
      "Path is: [8, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 6, 6, 2, 8, 8, 4, 5, 4, 1, 1];P and R: -4857;Capacity: 100;Epsilon: 0.9710927129400347\n",
      "Path is: [6, 2, 1, 1, 2, 2, 7, 4, 4, 5, 1, 1, 2, 7, 6, 1, 1, 9, 7, 2];P and R: -4355;Capacity: 99;Epsilon: 0.9704455335485082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|8                                                                                                                                                                   | 49/10000 [00:00<04:32, 36.57episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is: [8, 4, 1, 1, 1, 1, 7, 1, 1, 2, 5, 4, 1, 1, 3, 7, 1, 1, 5, 8];P and R: -4149;Capacity: 100;Epsilon: 0.9697987854661236\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [3, 2, 3, 9, 5, 3, 8, 9, 4, 4, 6, 1, 1, 1, 1, 8, 9, 8, 6, 7];P and R: 1747;Capacity: 88;Epsilon: 0.9691524684054375\n",
      "Path is: [4, 6, 7, 9, 3, 6, 2, 2, 4, 1, 1, 9, 5, 1, 1, 9, 1, 1, 3, 5];P and R: -3864;Capacity: 100;Epsilon: 0.9685065820791976\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 3, 1, 1, 3, 2, 8, 3, 4, 9, 6, 7];P and R: 3461;Capacity: 16;Epsilon: 0.9678611262003435\n",
      "Path is: [2, 3, 6, 1, 1, 8, 6, 6, 7, 9, 9, 1, 1, 5, 8, 6, 6, 6, 5, 5];P and R: -5436;Capacity: 86;Epsilon: 0.9672161004820059\n",
      "Path is: [4, 9, 3, 5, 6, 1, 1, 6, 4, 8, 8, 6, 2, 6, 3, 4, 9, 8, 2, 5];P and R: -3077;Capacity: 74;Epsilon: 0.9665715046375066\n",
      "Path is: [4, 6, 7, 9, 9, 7, 2, 6, 7, 4, 4, 3, 1, 1, 4, 4, 1, 1, 7, 6];P and R: -4444;Capacity: 100;Epsilon: 0.9659273383803587\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [2, 9, 5, 6, 5, 4, 3, 2, 4, 5, 5, 5, 5, 8, 2, 6, 3, 6, 2, 7];P and R: 1544;Capacity: 2;Epsilon: 0.965283601424266\n",
      "Path is: [1, 1, 6, 7, 5, 3, 5, 6, 4, 2, 8, 8, 1, 1, 2, 4, 8, 6, 2, 1, 1];P and R: -3951;Capacity: 100;Epsilon: 0.9646402934831231\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 6, 5, 9, 2, 2, 2, 6, 5, 6, 7, 2, 5, 2, 8, 8, 7, 5, 9, 4];P and R: 1537;Capacity: 2;Epsilon: 0.9639974142710154\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [6, 7, 6, 9, 8, 3, 2, 2, 9, 5, 9, 4];P and R: 3359;Capacity: 2;Epsilon: 0.9633549635022187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  1%|9                                                                                                                                                                   | 57/10000 [00:01<03:57, 41.84episodes/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|#                                                                                                                                                                   | 62/10000 [00:01<04:03, 40.74episodes/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Satisfied***\n",
      "Path is: [9, 2, 6, 7, 1, 1, 5, 4, 5, 6, 8];P and R: 3564;Capacity: 47;Epsilon: 0.9627129408911995\n",
      "Path is: [7, 8, 5, 8, 2, 7, 2, 8, 9, 3, 2, 2, 2, 5, 5, 9, 2, 2, 4, 7];P and R: -4182;Capacity: 26;Epsilon: 0.9620713461526144\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [5, 3, 1, 1, 6, 3, 1, 1, 8, 2, 9, 7, 7, 1, 1, 7, 2, 4];P and R: 1536;Capacity: 79;Epsilon: 0.96143017900131\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [7, 8, 4, 6, 3, 6, 5, 9, 9, 5, 7, 4, 4, 7, 2];P and R: 2552;Capacity: 2;Epsilon: 0.9607894391523232\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [3, 6, 1, 1, 9, 6, 4, 8, 2, 3, 7, 1, 1, 7, 3, 5];P and R: 2640;Capacity: 86;Epsilon: 0.9601491263208808\n",
      "Path is: [5, 7, 7, 1, 1, 3, 6, 9, 5, 6, 3, 3, 3, 8, 1, 1, 9, 2, 8, 9];P and R: -4466;Capacity: 92;Epsilon: 0.9595092402223991\n",
      "\n",
      "***Satisfied***\n",
      "Path is: [1, 1, 1, 1, 2, 8, 6, 9, 9, 7, 2, 1, 1, 2, 5, 3, 6, 8, 3, 4];P and R: 1353;Capacity: 65;Epsilon: 0.9588697805724845\n"
     ]
    }
   ],
   "source": [
    "show_preview = False #using this will make it slow. Don't care about this now.\n",
    "aggregrate_stats_every = 50\n",
    "start_decaying = True #making this True ignores the conditional decaying - Notes 2.a\n",
    "\n",
    "x_axis = list(range(environment.no_of_episodes+1))\n",
    "epsilon_list = []\n",
    "rewards_list = []\n",
    "path_length_list = []\n",
    "\n",
    "epsilon_decaying_factor = 1500\n",
    "\n",
    "\n",
    "# iterate over the episodes\n",
    "for episode in tqdm(range(1, environment.no_of_episodes + 1), ascii=True, unit='episodes'):\n",
    "    #print(episode)\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    # Reset environment and get initial state\n",
    "    current_state = environment.reset_environment()\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > environment.epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "            #print(\"action=\"+str(action)+\"\\n++++++\")\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(1, environment.action_space)\n",
    "\n",
    "        if action != 0: #JUST IGNORING 0. MIGHT BE STUPID!\n",
    "            new_state, reward, done, completed = environment.step(action)\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "        if show_preview and not episode % aggregrate_stats_every:\n",
    "            env.render()\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "        # Decay the Epsilon\n",
    "        if completed:\n",
    "            start_decaying = True\n",
    "        \n",
    "        if start_decaying:\n",
    "            # environment.epsilon = 0.001\n",
    "            if environment.epsilon > environment.min_epsilon:\n",
    "                # environment.epsilon *= environment.epsilon_decay\n",
    "                environment.epsilon = np.exp(-episode/epsilon_decaying_factor)\n",
    "                environment.epsilon = max(environment.min_epsilon, environment.epsilon)\n",
    "        # print(\".\",end=\"\")\n",
    "    epsilon_list.append(environment.epsilon)\n",
    "    rewards_list.append(episode_reward)\n",
    "    path_length_list.append(len(environment.truck.path))\n",
    "    \n",
    "    print(\"Path is:\", end=\" \")\n",
    "    print(environment.truck.path, end=\";\")\n",
    "    print(\"P and R: \"+str(episode_reward), end=\";\")\n",
    "    print(\"Capacity: \"+str(environment.truck.capacity), end=\";\")\n",
    "    print(\"Epsilon: \"+str(environment.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBaJVfaDzf5x"
   },
   "outputs": [],
   "source": [
    "# # problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/A/A-n32-k5.vrp')\n",
    "# problem = tsplib95.load_problem('/content/drive/My Drive/HW Assignments/Sem 2/ADBI/Vrp-All/_singleTruck/A-n32-k5_2.vrp')\n",
    "# nx_graph = problem.get_graph()\n",
    "# edge_list = list(problem.get_edges()) #[(,)]N\n",
    "# node_positions = problem.node_coords #dict\n",
    "# node_demands = copy.deepcopy(problem.demands)\n",
    "# truck_capacity = problem.capacity\n",
    "# Try rewarding for each node too i.e incentivize hopping!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWLdEBhZLRuR"
   },
   "outputs": [],
   "source": [
    "# node_demands[7] = 0\n",
    "# print(node_demands)\n",
    "# print(problem.demands)\n",
    "# print(sum(list(node_demands.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1u2PzUxLYsO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x236945c2780>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(10000))\n",
    "# print(x)\n",
    "k = 1500\n",
    "y = [float(np.exp(-i/k)) for i in x]\n",
    "# print(y)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis,epsilon_list,'r')\n",
    "plt.plot(x_axis,rewards_list,'g')\n",
    "plt.plot(x_axis,path_length_list,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = tsplib95.load_problem('Vrp-All/_singleTruck/A-n32-k5_3.vrp')\n",
    "problem.wfunc(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (3, 2), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "class keka:\n",
    "    def test(self,i):\n",
    "        return i[1]\n",
    "    a = [(1,3),(2,1),(3,2)]\n",
    "    def uppu(self):\n",
    "        self.a.sort(reverse=True,key=self.test)\n",
    "        return self.a\n",
    "    \n",
    "k = keka()\n",
    "print(k.uppu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vrp_dqn_update.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
